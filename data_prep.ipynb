{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "ep_path = f\"data/{anime}/episodes/\"\n",
    "abs_ep_path = os.path.abspath(ep_path)\n",
    "episodes = [os.path.join(ep_path, file) for file in os.listdir(ep_path)][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "\n",
    "# Iframes and Pframes\n",
    "filename = episodes\n",
    "save_path = f\"data/{anime}/frames/ep4\"\n",
    "\n",
    "def get_frame_types(video_fn):\n",
    "\tcommand = 'ffprobe -v error -show_entries frame=pict_type -of default=noprint_wrappers=1'.split()\n",
    "\tout = subprocess.check_output(command + [video_fn]).decode()\n",
    "\tframe_types = out.replace('pict_type=','').split()\n",
    "\treturn zip(range(len(frame_types)), frame_types)\n",
    "\n",
    "def save_keyframes(video_fn, out_path):\n",
    "\tframe_types = get_frame_types(video_fn)\n",
    "\ti_frames = [x[0] for x in frame_types if x[1]=='P']\n",
    "\tif i_frames:\n",
    "\t\tbasename = os.path.splitext(os.path.basename(video_fn))[0]\n",
    "\t\tcap = cv2.VideoCapture(video_fn)\n",
    "\t\tfor frame_no in i_frames:\n",
    "\t\t\tcap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "\t\t\tret, frame = cap.read()\n",
    "\t\t\toutname = os.path.join(out_path, f\"{basename}_p_frame_{str(frame_no)}.jpg\")\n",
    "\t\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\tcv2.imwrite(outname, frame)\n",
    "\t\tcap.release()\n",
    "\telse:\n",
    "\t\tprint ('No I-frames in '+video_fn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tsave_keyframes(filename, save_path)\n",
    "# Pframes: 8m, Iframes: 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastest way to get Iframes\n",
    "import av\n",
    "\n",
    "content = episodes\n",
    "with av.open(content) as container:\n",
    "\t# Signal that we only want to look at keyframes.\n",
    "\tstream = container.streams.video[0]\n",
    "\tstream.codec_context.skip_frame = \"NONREF\"\n",
    "\tstream.codec_context.thread_type = \"FRAME\"\n",
    "\n",
    "\tfor frame in container.decode(stream):\n",
    "\n",
    "\t\t# We use `frame.pts` as `frame.index` won't make must sense with the `skip_frame`.\n",
    "\t\tsave_path = os.path.join(f\"mta_data/{anime}/frames/ep3\", f\"fma_key_frame_{frame.pts}.jpg\")\n",
    "\t\t# frame.to_image().save(save_path)\n",
    "\t\t\n",
    "\t\t # Convert the frame to grayscale.\n",
    "\t\tgray_frame = frame.to_image().convert(\"L\")\n",
    "\t\tgray_frame.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Hashing, not really used\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "def with_ztransform_preprocess(hashfunc, hash_size=8):\n",
    "\tdef function(path):\n",
    "\t\timage = Image.open(path)\n",
    "\t\timage = image.convert(\"L\").resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "\t\tdata = image.getdata()\n",
    "\t\tquantiles = np.arange(100)\n",
    "\t\tquantiles_values = np.percentile(data, quantiles)\n",
    "\t\tzdata = (np.interp(data, quantiles_values, quantiles) / 100 * 255).astype(np.uint8)\n",
    "\t\timage.putdata(zdata)\n",
    "\t\treturn hashfunc(image)\n",
    "\treturn function\n",
    "\n",
    "dhash_z_transformed = with_ztransform_preprocess(imagehash.dhash, hash_size = 8)\n",
    "out_folder = f\"mta_data/frames/{anime}/Pframes_filtered\"\n",
    "pics = [os.path.join(out_folder, file) for file in os.listdir(out_folder)]\n",
    "hashes = [dhash_z_transformed(pic) for pic in pics]\n",
    "\n",
    "df = pd.DataFrame({\"image_ids\": pics, \"hash_values\": hashes, })\n",
    "df_clean = df[df.duplicated(['hash_values'], keep=False)]\n",
    "for image in df_clean[\"image_ids\"].values:\n",
    "\tos.remove(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def remove_text(model_path, original_img, debug_display=False):\n",
    "\ttf.data.experimental.enable_debug_mode()\n",
    "\t# Load the model\n",
    "\tmodel = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "\t# Pre-process the image\n",
    "\theight, width = original_img.shape[:2]\n",
    "\n",
    "\timg = np.expand_dims(original_img, axis=-1)  # add an extra dimension for the channel\n",
    "\timg = np.repeat(img, 3, axis=-1)  # repeat the image along the channel dimension\n",
    "\timg = resize(img, (768,512), anti_aliasing=True, preserve_range=True)\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\timg = img/255\n",
    "\n",
    "\t# Use the model to make a prediction\n",
    "\tp = model.predict(img)\n",
    "\n",
    "\tmask = p[0,:,:,0]\n",
    "\n",
    "\tmask = np.round(mask, 0)\n",
    "\n",
    "\t# Return the image to its original dimensions\n",
    "\tmask_img = Image.fromarray(mask)\n",
    "\tmask_img = mask_img.resize((width,height), resample=Image.Resampling.BICUBIC)\n",
    "\tmask = np.asarray(mask_img)\n",
    "\t\n",
    "\t# Convert the mask to a boolean array\n",
    "\tmask = mask == 1\n",
    "\n",
    "\tno_text = original_img.copy()\n",
    "\t# Set the pixels in the original image to 1 wherever the mask is 0\n",
    "\tno_text[mask] = 255\n",
    "\t# no_text = np.where(mask, mask, no_text)\n",
    "\n",
    "\t# Increase the contrast of the image\n",
    "\tno_text = exposure.rescale_intensity(no_text, out_range=(0, 255))\n",
    "\tno_text = cv2.normalize(src=no_text, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "\tif debug_display == True:\n",
    "\t\t# Display the resulting image\n",
    "\t\tplt.imshow(mask, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\t\tplt.imshow(original_img, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\t\tplt.imshow(no_text, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\telse:\n",
    "\t\treturn no_text\n",
    "\t\t\n",
    "# img_path = test[3]\n",
    "# img = cv2.imread(\"data/x5.png\", cv2.IMREAD_GRAYSCALE)\n",
    "# remove_text(model_path=\"0207_e500_std_model_4.h5\", original_img=img, debug_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kumiko import kumikolib\n",
    "import re\n",
    "\n",
    "def enough_info(image, thresh=95):\n",
    "\timage = cv2.equalizeHist(image)\n",
    "\t_, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\ttotal_pix = image.shape[0] * image.shape[1]\n",
    "\twhite = np.sum(image >= 254)\n",
    "\twhite_percent = round((white / total_pix) * 100)\n",
    "\tblack = np.sum(image <= 1)\n",
    "\tblack_percent = round((black / total_pix) * 100)\n",
    "\t# print(black_percent, white_percent)\n",
    "\tif (white_percent < thresh and black_percent < thresh):\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\n",
    "def panel_extractor(im_path, save_path):\n",
    "\tk = kumikolib.Kumiko()\n",
    "\t# Open the input image\n",
    "\t# img = cv2.imread(im_path, cv2.IMREAD_GRAYSCALE)\n",
    "\timg = Image.open(im_path)\n",
    "\t# Check the mode of the image\n",
    "\tif img.mode == 'RGB':\n",
    "\t\treturn\n",
    "\timg = img.convert('L')\n",
    "\timg = np.array(img)\n",
    "\timg = remove_text(model_path=\"0207_e500_std_model_4.h5\", original_img=img)\n",
    "\tim_size = img.shape[0] * img.shape[1]\n",
    "\n",
    "\tif enough_info(img, thresh=95):\n",
    "\t\tinfo = k.parse_image(im_path, image=img)\n",
    "\t\tpanels = info[\"panels\"]\n",
    "\n",
    "\t\tpanel_percent = 0.05\n",
    "\t\tfor index, panel in enumerate(panels):\n",
    "\t\t\tcrop_img = img[panel[1]:panel[1]+panel[3], panel[0]:panel[0]+panel[2]]\n",
    "\t\t\tif (crop_img.shape[0] * crop_img.shape[1] > panel_percent * im_size):\n",
    "\t\t\t\toutput_path = os.path.join(save_path, 'panel_{}-{}.png'.format(re.search(r'\\D*(\\d+)\\.\\D*', im_path).group(1), index))\n",
    "\t\t\t\tcv2.imwrite(output_path, crop_img)\n",
    "\t\t\t\t# pil_img = Image.fromarray(crop_img)\n",
    "\t\t\t\t# res = pil_resize(pil_img)\n",
    "\t\t\t\t# res.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "anime = 'fma'\n",
    "manga_path = f\"data/{anime}/manga\"\n",
    "panel_path = f\"data/{anime}/panels\"\n",
    "manga_chapters = natsorted(os.listdir(manga_path))\n",
    "manga = {}\n",
    "for chapter in manga_chapters:\n",
    "\tch_path = os.path.join(manga_path, chapter)\n",
    "\tmanga[chapter] = natsorted([os.path.join(ch_path, file) for file in os.listdir(ch_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f\"data\"\n",
    "chapter = \"ch5\"\n",
    "save_path = os.path.join(panel_path, chapter)\n",
    "for mng in manga[chapter]:\n",
    "\tpanel_extractor(mng, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### panel-anime correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "frame_path = f\"data/fma_Pframes_resnet_ep3\"\n",
    "anime_frames = [os.path.join(frame_path, file) for file in os.listdir(frame_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4062) tensor(0.1713)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_mean_std(image_paths, my_batch_size=32):\n",
    "\t# Define a dataset that reads the images one at a time\n",
    "\tclass ImageDataset(torch.utils.data.Dataset):\n",
    "\t\tdef __init__(self, image_paths, transform=None):\n",
    "\t\t\tself.image_paths = image_paths\n",
    "\t\t\tself.transform = transform\n",
    "\n",
    "\t\tdef __getitem__(self, index):\n",
    "\t\t\timage = Image.open(self.image_paths[index])\n",
    "\t\t\tif self.transform:\n",
    "\t\t\t\timage = self.transform(image)\n",
    "\t\t\treturn image\n",
    "\n",
    "\t\tdef __len__(self):\n",
    "\t\t\treturn len(self.image_paths)\n",
    "\n",
    "\t# Define a transform that converts the images to tensors\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t])\n",
    "\n",
    "\t# Create a dataset and a dataloader\n",
    "\tdataset = ImageDataset(image_paths, transform=transform)\n",
    "\tdataloader = DataLoader(dataset, batch_size=my_batch_size, num_workers=4)\n",
    "\n",
    "\t# Calculate mean and std\n",
    "\tmean = 0.0\n",
    "\tstd = 0.0\n",
    "\tcount = 0\n",
    "\tfor images in dataloader:\n",
    "\t\timages = images.view(images.size(0), -1)\n",
    "\t\tmean += images.mean(dim=1).sum()\n",
    "\t\tstd += images.std(dim=1).sum()\n",
    "\t\tcount += images.size(0)\n",
    "\tmean /= count\n",
    "\tstd /= count\n",
    "\n",
    "\treturn mean, std\n",
    "\n",
    "image_paths = anime_frames\n",
    "mean, std = calculate_mean_std(image_paths)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def pil_resize(img, width, height):\n",
    "\t# Get the original aspect ratio\n",
    "\toriginal_aspect = img.width / img.height\n",
    "\n",
    "\t# Determine the new width and height\n",
    "\tnew_width = width\n",
    "\tnew_height = int(new_width / original_aspect)\n",
    "\n",
    "\t# Resize the image\n",
    "\tresized_im = img.resize((new_width, new_height), resample=Image.Resampling.BICUBIC)\n",
    "\n",
    "\t# Check if the image is larger than width*height\n",
    "\tif img.width > width or img.height > height:\n",
    "\t\t# Resize the image using thumbnail method\n",
    "\t\tresized_im.thumbnail((width, height), resample=Image.Resampling.BICUBIC)\n",
    "\n",
    "\t# Create a new image with white background (255 represents white)\n",
    "\tnew_im = Image.new(\"L\", (width, height), 255)\n",
    "\n",
    "\t# Paste the resized image on center of the new image\n",
    "\tleft = (width - resized_im.width) // 2\n",
    "\ttop = (height - resized_im.height) // 2\n",
    "\tnew_im.paste(resized_im, (left, top))\n",
    "\t\n",
    "\treturn new_im\n",
    "\n",
    "def my_norm(img):\n",
    "\tnew_im = np.array(img)\n",
    "\tnew_im = exposure.rescale_intensity(new_im, out_range=(0, 255))\n",
    "\tnew_im = cv2.normalize(src=new_im, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\t# new_im = Image.fromarray(new_im)\n",
    "\treturn new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as Models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the mean and standard deviation values for ImageNet\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# Define the mean and standard deviation values from my images\n",
    "mean_std = (mean, std)\n",
    "\n",
    "# Create the normalization transform\n",
    "transform = transforms.Compose([\n",
    "\t# transforms.Grayscale(),\n",
    "\t# transforms.Resize((224, 224)),\n",
    "\ttransforms.Lambda(lambda x: pil_resize(x, 224, 224)),\n",
    "\ttransforms.Lambda(lambda x: my_norm(x)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=mean, std=std),\n",
    "\ttransforms.Lambda(lambda x: x.expand(1, 3, -1, -1).view(1, 3, 224, 224))\n",
    "])\n",
    "\n",
    "def extract_features(frame_path):\n",
    "\tframe = Image.open(frame_path)\n",
    "\timg_tensor = transform(frame)\n",
    "\twith torch.no_grad():\n",
    "\t\tframe_features = model(img_tensor).detach().numpy()\n",
    "\treturn frame_features\n",
    "\n",
    "# Load the ResNet50 model from PyTorch's model zoo\n",
    "model = Models.resnet50(weights=Models.ResNet50_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Extract the features from the model's second-to-last fully-connected layer\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Read the anime frame images\n",
    "frame_paths = anime_frames\n",
    "\n",
    "# Extract the features for all anime frames\n",
    "torch.set_num_threads(1)\n",
    "with Pool(processes=cpu_count()) as p:\n",
    "\tframe_features_list = list(p.imap(extract_features, frame_paths))\n",
    "\n",
    "frame_features_list_squeezed = [np.squeeze(f) for f in frame_features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test some different pretrained models\n",
    "import torch\n",
    "import torchvision.models as Models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the mean and standard deviation values from my images\n",
    "mean_std = (mean, std)\n",
    "\n",
    "# Create the normalization transform\n",
    "transform = transforms.Compose([\n",
    "\t# transforms.Grayscale(),\n",
    "\t# transforms.Resize((224, 224)),\n",
    "\ttransforms.Lambda(lambda x: pil_resize(x, 299, 299)),\n",
    "\ttransforms.Lambda(lambda x: my_norm(x)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=mean, std=std),\n",
    "\ttransforms.Lambda(lambda x: x.expand(1, 3, -1, -1).view(1, 3, 299, 299))\n",
    "])\n",
    "\n",
    "def extract_features(frame_path):\n",
    "\tframe = Image.open(frame_path)\n",
    "\timg_tensor = transform(frame)\n",
    "\twith torch.no_grad():\n",
    "\t\tframe_features = model(img_tensor).detach().numpy()\n",
    "\treturn frame_features\n",
    "\n",
    "# Load the ResNet50 model from PyTorch's model zoo\n",
    "model = Models.densenet201(weights=Models.DenseNet201_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Read the anime frame images\n",
    "frame_paths = anime_frames\n",
    "\n",
    "# Extract the features for all anime frames\n",
    "torch.set_num_threads(1)\n",
    "with Pool(processes=cpu_count()) as p:\n",
    "\tframe_features_list = list(p.imap(extract_features, frame_paths))\n",
    "\n",
    "frame_features_list_squeezed = [np.squeeze(f) for f in frame_features_list]\n",
    "# Densenet201, resnet152 > Inception_v3, Xception, EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trees 14\n",
      "log num_neighbors 443\n"
     ]
    }
   ],
   "source": [
    "# Choose most significant frames, by deleting the rest in the frame_path folder\n",
    "import annoy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "frame_path = f\"mta_data/frames/{anime}/Pframes_resnet\"\n",
    "frame_paths = [os.path.join(frame_path, file) for file in os.listdir(frame_path)]\n",
    "\n",
    "def calculate_similarity(vector1, vector2):\n",
    "\t# Convert the vectors to PyTorch tensors\n",
    "\tvector1 = torch.tensor(vector1)\n",
    "\tvector2 = torch.tensor(vector2)\n",
    "\t\n",
    "\t# Flatten the tensors to 1D\n",
    "\tvector1 = vector1.view(-1)\n",
    "\tvector2 = vector2.view(-1)\n",
    "\t\n",
    "\t# Calculate the dot product and norms of the two feature vectors\n",
    "\tdot_product = torch.dot(vector1, vector2)\n",
    "\tnorm1 = torch.norm(vector1)\n",
    "\tnorm2 = torch.norm(vector2)\n",
    "\t\n",
    "\t# Calculate the cosine similarity between the two feature vectors\n",
    "\tsimilarity = dot_product / (norm1 * norm2)\n",
    "\treturn similarity.tolist()\n",
    "\n",
    "def calculate_optimal_trees(n_samples, accuracy):\n",
    "\t# Calculate the optimal number of trees based on the size of the dataset and the desired accuracy\n",
    "\tn_trees = int(np.round(np.log(n_samples) / np.log(2) / accuracy))\n",
    "\treturn n_trees\n",
    "\n",
    "# Build the Annoy index from the frame feature vectors\n",
    "annoy_index = annoy.AnnoyIndex(len(frame_features_list_squeezed[0]), metric='angular')\n",
    "for i, frame_features in enumerate(frame_features_list_squeezed):\n",
    "\tannoy_index.add_item(i, frame_features)\n",
    "\n",
    "# Calculate the sample length of the dataset\n",
    "n_samples = len(frame_features_list_squeezed)\n",
    "\n",
    "# Set the desired accuracy of the search\n",
    "accuracy = 0.9\n",
    "# Set the similarity threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Calculate the optimal number of trees for a dataset with n_samples and an accuracy of accuracy\n",
    "n_trees = calculate_optimal_trees(n_samples, accuracy)\n",
    "print(\"n_trees\", n_trees)\n",
    "annoy_index.build(n_trees)\n",
    "\n",
    "# Set the number of nearest neighbors to search for\n",
    "num_neighbors = int(n_samples * math.log(1 / accuracy) * 0.5)\n",
    "print(\"log num_neighbors\", num_neighbors)\n",
    "\n",
    "# Create a list to store the indexes of the frames to delete\n",
    "frames_to_delete = []\n",
    "\n",
    "# Iterate through the list of frame feature vectors\n",
    "for i, frame_features in enumerate(frame_features_list_squeezed):\n",
    "\t# Check if the current frame has already been added to the list of frames to delete\n",
    "\tif i in frames_to_delete:\n",
    "\t\tcontinue\n",
    "\t# Find the nearest neighbors of the current frame in the Annoy index\n",
    "\tindices = annoy_index.get_nns_by_item(i, num_neighbors)\n",
    "\t# Iterate through the nearest neighbors\n",
    "\tfor j in indices:\n",
    "\t\t# Skip the current frame\n",
    "\t\tif i == j:\n",
    "\t\t\tcontinue\n",
    "\t\t# Calculate the similarity between the current frame and the next nearest neighbor\n",
    "\t\tsimilarity = calculate_similarity(frame_features, frame_features_list_squeezed[j])\n",
    "\t\t# If the similarity is above the threshold, add the nearest neighbor to the list of frames to delete\n",
    "\t\tif similarity > threshold:\n",
    "\t\t\tframes_to_delete.append(j)\n",
    "\n",
    "# Create a list to store the paths of the images to delete\n",
    "image_paths_to_delete = [frame_paths[i] for i in range(len(frame_paths)) if i in frames_to_delete]\n",
    "\n",
    "# Create a ThreadPoolExecutor with a fixed number of threads\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "\t # Submit the delete tasks to the executor\n",
    "\t for image_path in image_paths_to_delete:\n",
    "\t\t executor.submit(os.remove, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar frame for panel data/panel_3.png, data/fma_Pframes_resnet_ep3/fma_04_i_frame_6977.jpg (similarity: 0.6283)\n"
     ]
    }
   ],
   "source": [
    "def calculate_similarity(vector1, vector2):\n",
    "\t# Convert the vectors to PyTorch tensors\n",
    "\tvector1 = torch.tensor(vector1)\n",
    "\tvector2 = torch.tensor(vector2)\n",
    "\t\n",
    "\t# Flatten the tensors to 1D\n",
    "\tvector1 = vector1.view(-1)\n",
    "\tvector2 = vector2.view(-1)\n",
    "\t\n",
    "\t# Calculate the dot product and norms of the two feature vectors\n",
    "\tdot_product = torch.dot(vector1, vector2)\n",
    "\tnorm1 = torch.norm(vector1)\n",
    "\tnorm2 = torch.norm(vector2)\n",
    "\t\n",
    "\t# Calculate the cosine similarity between the two feature vectors\n",
    "\tsimilarity = dot_product / (norm1 * norm2)\n",
    "\treturn similarity.tolist()\n",
    "\n",
    "# Read the manga panel image\n",
    "panel_path = 'data/panel_3.png'\n",
    "panel = Image.open(panel_path)\n",
    "panel = transform(panel)\n",
    "panel_features = model(panel).detach().numpy()\n",
    "\n",
    "# Initialize the most similar frame and its similarity score\n",
    "most_similar_frame = None\n",
    "highest_similarity = -1\n",
    "\n",
    "# Compare the manga panel with all anime frames\n",
    "for i, frame_features in enumerate(frame_features_list):\n",
    "\tsimilarity = calculate_similarity(panel_features, frame_features)\n",
    "\tif similarity > highest_similarity:\n",
    "\t\thighest_similarity = similarity\n",
    "\t\tmost_similar_frame = frame_paths[i]\n",
    "\n",
    "# Print the most similar frame and its similarity score\n",
    "print(f\"Most similar frame for panel {panel_path}, {most_similar_frame} (similarity: {highest_similarity:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "826286f252ddcff787408661d182f2c63da5c69a1bdcb0cb2e03ac788e5f338e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
