{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import imageio.v3 as iio\n",
    "# from pymediainfo import MediaInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "ep_path = f\"mta_data/episodes/{anime}/\"\n",
    "abs_ep_path = os.path.abspath(f\"mta_data/episodes/{anime}/\")\n",
    "episodes = [os.path.join(ep_path, file) for file in os.listdir(ep_path)][2]\n",
    "\n",
    "# media_info = MediaInfo.parse(episodes[0])\n",
    "# duration_sec = media_info.tracks[0].duration / 1000\n",
    "# frame_rate = media_info.video_tracks[0].frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mta_data/episodes/fma/fma_04.mkv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iframes and Pframes\n",
    "filename = episodes\n",
    "\n",
    "def get_frame_types(video_fn):\n",
    "\tcommand = 'ffprobe -v error -show_entries frame=pict_type -of default=noprint_wrappers=1'.split()\n",
    "\tout = subprocess.check_output(command + [video_fn]).decode()\n",
    "\tframe_types = out.replace('pict_type=','').split()\n",
    "\treturn zip(range(len(frame_types)), frame_types)\n",
    "\n",
    "def save_i_keyframes(video_fn):\n",
    "\tframe_types = get_frame_types(video_fn)\n",
    "\ti_frames = [x[0] for x in frame_types if x[1]=='I']\n",
    "\tif i_frames:\n",
    "\t\tbasename = os.path.splitext(os.path.basename(video_fn))[0]\n",
    "\t\tcap = cv2.VideoCapture(video_fn)\n",
    "\t\tfor frame_no in i_frames:\n",
    "\t\t\tcap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "\t\t\tret, frame = cap.read()\n",
    "\t\t\toutname = os.path.join(f\"mta_data/frames/{anime}/Iframes_greys\", f\"{basename}_i_frame_{str(frame_no)}.jpg\")\n",
    "\t\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\tcv2.imwrite(outname, frame)\n",
    "\t\tcap.release()\n",
    "\telse:\n",
    "\t\tprint ('No I-frames in '+video_fn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tsave_i_keyframes(filename)\n",
    "# Pframes: 8m, Iframes: 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decord import VideoReader\n",
    "\n",
    "vr = VideoReader(episodes)\n",
    "key_indices = vr.get_key_indices()\n",
    "key_frames = vr.get_batch(key_indices)\n",
    "for index, frame in enumerate(key_frames.asnumpy()):\n",
    "\tsave_path = os.path.join(f\"mta_data/frames/{anime}/Iframes_greys\", f\"fma_key_frame_{index}.jpg\")\n",
    "\tcv2.imwrite(save_path, cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "\n",
    "content = episodes\n",
    "with av.open(content) as container:\n",
    "\t# Signal that we only want to look at keyframes.\n",
    "\tstream = container.streams.video[0]\n",
    "\tstream.codec_context.skip_frame = \"NONREF\"\n",
    "\n",
    "\tfor frame in container.decode(stream):\n",
    "\n",
    "\t\t# We use `frame.pts` as `frame.index` won't make must sense with the `skip_frame`.\n",
    "\t\tsave_path = os.path.join(f\"mta_data/frames/{anime}/av_NONREF_greys\", f\"fma_key_frame_{frame.pts}.jpg\")\n",
    "\t\t# frame.to_image().save(save_path)\n",
    "\t\t\n",
    "\t\t # Convert the frame to grayscale.\n",
    "\t\tgray_frame = frame.to_image().convert(\"L\")\n",
    "\t\tgray_frame.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "def with_ztransform_preprocess(hashfunc, hash_size=8):\n",
    "\tdef function(path):\n",
    "\t\timage = Image.open(path)\n",
    "\t\timage = image.convert(\"L\").resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "\t\tdata = image.getdata()\n",
    "\t\tquantiles = np.arange(100)\n",
    "\t\tquantiles_values = np.percentile(data, quantiles)\n",
    "\t\tzdata = (np.interp(data, quantiles_values, quantiles) / 100 * 255).astype(np.uint8)\n",
    "\t\timage.putdata(zdata)\n",
    "\t\treturn hashfunc(image)\n",
    "\treturn function\n",
    "\n",
    "dhash_z_transformed = with_ztransform_preprocess(imagehash.dhash, hash_size = 8)\n",
    "out_folder = f\"mta_data/frames/{anime}/Pframes_filtered\"\n",
    "pics = [os.path.join(out_folder, file) for file in os.listdir(out_folder)]\n",
    "hashes = [dhash_z_transformed(pic) for pic in pics]\n",
    "\n",
    "df = pd.DataFrame({\"image_ids\": pics, \"hash_values\": hashes, })\n",
    "df_clean = df[df.duplicated(['hash_values'], keep=False)]\n",
    "for image in df_clean[\"image_ids\"].values:\n",
    "\tos.remove(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "manga_path = f\"mta_data/manga/{anime}/\"\n",
    "manga = [os.path.join(manga_path, file) for file in os.listdir(manga_path)]\n",
    "test = manga[6:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def alternative_extractor(im_path, save_path, panel_percent=0.05):\n",
    "\timg = cv2.imread(im_path)\n",
    "\timgrey = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\tim_y = imgrey.shape[0]\n",
    "\tim_x = imgrey.shape[1]\n",
    "\n",
    "\tret,thresh1 = cv2.threshold(imgrey,127,255,cv2.THRESH_BINARY_INV)\n",
    "\tcontours,heirarchy = cv2.findContours(thresh1,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\tfor contour in contours:\n",
    "\t\thull = cv2.convexHull(contour)\n",
    "\t\tcv2.drawContours(imgrey,[hull],-1,0,-1)\n",
    "\n",
    "\tret,thresh1 = cv2.threshold(imgrey,127,255,cv2.THRESH_BINARY_INV)\n",
    "\tcontours,heirarchy = cv2.findContours(thresh1,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\t# Sort the contours by their y coordinate\n",
    "\tsorted_contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[1])\n",
    "\n",
    "\tpanel_index = 0\n",
    "\tfor contour in sorted_contours:\n",
    "\t\t# Find the bounding rectangle of the contour\n",
    "\t\tx, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "\t\t# Crop the image using Numpy slicing\n",
    "\t\tpanel = img[y:y+h, x:x+w]\n",
    "\t\tpanel_y = panel.shape[0]\n",
    "\t\tpanel_x = panel.shape[1]\n",
    "\n",
    "\t\tpp = panel_percent\n",
    "\t\tif (panel_y > pp * im_y) and (panel_x > pp * im_x):\n",
    "\t\t\t# Save the cropped panel image, using the current panel index\n",
    "\t\t\toutput_path = os.path.join(save_path, f'panel_{panel_index}.png')\n",
    "\t\t\tcv2.imwrite(output_path, panel)\n",
    "\t\t\tpanel_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kumiko import kumikolib\n",
    "\n",
    "def kumiko_extractor(im_path, save_path, white_thresh=95):\n",
    "\tk = kumikolib.Kumiko()\n",
    "\t# info = k.parse_images(test)\n",
    "\tinfo = k.parse_image(im_path)\n",
    "\tpanels = info[\"panels\"]\n",
    "\timg = cv2.imread(im_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\tfor index, panel in enumerate(panels):\n",
    "\t\tcrop_img = img[panel[1]:panel[1]+panel[3], panel[0]:panel[0]+panel[2]]\n",
    "\n",
    "\t\twhite = np.sum(crop_img >= 254)\n",
    "\t\ttotal_pix = crop_img.shape[0] * crop_img.shape[1]\n",
    "\t\twhite_percent = (white / total_pix) * 100\n",
    "\t\tif (white_percent < white_thresh):\n",
    "\t\t\toutput_path = os.path.join(save_path, f'panel_{index}.png')\n",
    "\t\t\tcv2.imwrite(output_path, crop_img)\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\treturn 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = test[3]\n",
    "save_path = f\"mta_data/panels/{anime}\"\n",
    "\n",
    "if not kumiko_extractor(im_path, save_path, white_thresh=95):\n",
    "\talternative_extractor(im_path, save_path, panel_percent=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### panel-anime correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "frame_path = f\"mta_data/frames/{anime}/Pframes_greys\"\n",
    "anime_frames = [os.path.join(frame_path, file) for file in os.listdir(frame_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def with_ztransform_preprocess(hashfunc, image, hash_size=8):\n",
    "\timage = image.convert(\"L\").resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "\tdata = image.getdata()\n",
    "\tquantiles = np.arange(100)\n",
    "\tquantiles_values = np.percentile(data, quantiles)\n",
    "\tzdata = (np.interp(data, quantiles_values, quantiles) / 100 * 255).astype(np.uint8)\n",
    "\timage.putdata(zdata)\n",
    "\n",
    "\thash = hashfunc(image)\n",
    "\n",
    "\treturn hash\n",
    "\n",
    "def compare_panel_to_frames(hashfunc, panel, frame_hashes, anime_frames, hash_size=8):\n",
    "\tpanel_hash = with_ztransform_preprocess(hashfunc, panel, hash_size)\n",
    "\n",
    "\tpanel_distances = []\n",
    "\t# Compare the manga panel to each anime frame\n",
    "\tfor frame_hash in frame_hashes:\n",
    "\t\tdistance = panel_hash - frame_hash\n",
    "\t\tpanel_distances.append(distance)\n",
    "\t# Find the index of the minimum distance for the current manga panel\n",
    "\tmin_index = panel_distances.index(min(panel_distances))\n",
    "\t# The min_index will be the index of the anime frame that is most similar to the current manga panel\n",
    "\tmost_similar_frame = anime_frames[min_index]\n",
    "\t# Return the most similar frame and the minimum distance\n",
    "\treturn (most_similar_frame, min(panel_distances))\n",
    "\n",
    "panels = ['mta_data/panels/fma/panel_4.jpg']\n",
    "\n",
    "# Calculate the hash function for the anime frames using pHash\n",
    "hashfunc = imagehash.phash\n",
    "\n",
    "# Calculate the hashes of the anime frames\n",
    "frame_hashes = []\n",
    "for frame_path in anime_frames:\n",
    "\tframe = Image.open(frame_path)\n",
    "\tframe_hash = with_ztransform_preprocess(hashfunc, frame)\n",
    "\tframe_hashes.append(frame_hash)\n",
    "\n",
    "# Initialize an empty list to store the distances\n",
    "distances = []\n",
    "\n",
    "# Iterate over the manga panels\n",
    "for panel_path in panels:\n",
    "\tpanel = Image.open(panel_path)\n",
    "\t# Compare the manga panel to the anime frames\n",
    "\tresult = compare_panel_to_frames(hashfunc, panel, frame_hashes, anime_frames)\n",
    "\t# Append the result to the distances list\n",
    "\tdistances.append(result)\n",
    "\n",
    "img = mpimg.imread(distances[0][0])\n",
    "imgplot = plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_frame = cv2.imread(anime_frame_path)\n",
    "frame_height, frame_width, _ = anime_frame.shape\n",
    "manga_panel_gray = cv2.resize(manga_panel_gray, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_mean_std(image_paths, my_batch_size=32):\n",
    "\t# Define a dataset that reads the images one at a time\n",
    "\tclass ImageDataset(torch.utils.data.Dataset):\n",
    "\t\tdef __init__(self, image_paths, transform=None):\n",
    "\t\t\tself.image_paths = image_paths\n",
    "\t\t\tself.transform = transform\n",
    "\n",
    "\t\tdef __getitem__(self, index):\n",
    "\t\t\timage = Image.open(self.image_paths[index])\n",
    "\t\t\tif self.transform:\n",
    "\t\t\t\timage = self.transform(image)\n",
    "\t\t\treturn image\n",
    "\n",
    "\t\tdef __len__(self):\n",
    "\t\t\treturn len(self.image_paths)\n",
    "\n",
    "\t# Define a transform that converts the images to tensors\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t])\n",
    "\n",
    "\t# Create a dataset and a dataloader\n",
    "\tdataset = ImageDataset(image_paths, transform=transform)\n",
    "\tdataloader = DataLoader(dataset, batch_size=my_batch_size, num_workers=4)\n",
    "\n",
    "\t# Calculate mean and std\n",
    "\tmean = 0.0\n",
    "\tstd = 0.0\n",
    "\tcount = 0\n",
    "\tfor images in dataloader:\n",
    "\t\timages = images.view(images.size(0), -1)\n",
    "\t\tmean += images.mean(dim=1).sum()\n",
    "\t\tstd += images.std(dim=1).sum()\n",
    "\t\tcount += images.size(0)\n",
    "\tmean /= count\n",
    "\tstd /= count\n",
    "\n",
    "\treturn mean, std\n",
    "\n",
    "image_paths = anime_frames\n",
    "mean, std = calculate_mean_std(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import spatial\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def resize(img, size):\n",
    "\t# Check if the image is a tensor\n",
    "\tif isinstance(img, torch.Tensor):\n",
    "\t\t# Remove the extra dimensions if the image is 4D\n",
    "\t\tif img.ndim == 4:\n",
    "\t\t\timg = img.squeeze(0)\n",
    "\t\t# Convert the tensor to a PIL image\n",
    "\t\timg = transforms.functional.to_pil_image(img)\n",
    "\t# Resize the image\n",
    "\timg = img.resize(size)\n",
    "\treturn img\n",
    "\n",
    "def get_feature_vector(img, model, mean_std):\n",
    "\t# Resize the image to the input size of the model\n",
    "\timg = resize(img, (224, 224))\n",
    "\t# Convert the image to a tensor and normalize it using the model's mean and std\n",
    "\timg_tensor = transforms.functional.to_tensor(img)\n",
    "\tif img_tensor.ndim == 4:\n",
    "\t\t# Remove the extra dimensions\n",
    "\t\timg_tensor = img_tensor.squeeze(0)\n",
    "\timg_tensor = img_tensor.view(1, 3, 224, 224)\n",
    "\timg_tensor = transforms.functional.normalize(img_tensor, mean=mean_std[0], std=mean_std[1])\n",
    "\t# Extract the features from the image using the model\n",
    "\tfeatures = model(img_tensor).detach().numpy()\n",
    "\treturn features\n",
    "\n",
    "def calculate_similarity(vector1, vector2):\n",
    "\t# Convert the vectors to PyTorch tensors\n",
    "\tvector1 = torch.tensor(vector1)\n",
    "\tvector2 = torch.tensor(vector2)\n",
    "\t\n",
    "\t# Flatten the tensors to 1D\n",
    "\tvector1 = vector1.view(-1)\n",
    "\tvector2 = vector2.view(-1)\n",
    "\t\n",
    "\t# Calculate the dot product and norms of the two feature vectors\n",
    "\tdot_product = torch.dot(vector1, vector2)\n",
    "\tnorm1 = torch.norm(vector1)\n",
    "\tnorm2 = torch.norm(vector2)\n",
    "\t\n",
    "\t# Calculate the cosine similarity between the two feature vectors\n",
    "\tsimilarity = dot_product / (norm1 * norm2)\n",
    "\treturn similarity.tolist()\n",
    "\n",
    "\n",
    "# Define the mean and standard deviation values for ImageNet\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# Define the mean and standard deviation values from my images\n",
    "mean_std = (mean, std)\n",
    "\n",
    "# Create the normalization transform\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Load the ResNet50 model from PyTorch's model zoo\n",
    "model = torchvision.models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Extract the features from the model's second-to-last fully-connected layer\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Read the anime frame images\n",
    "frame_paths = anime_frames\n",
    "\n",
    "# Extract the features for all anime frames\n",
    "frame_features_list = []\n",
    "for i, frame_path in enumerate(frame_paths):\n",
    "\tframe = cv2.imread(frame_path)\n",
    "\tframe = transform(frame).unsqueeze(0)\n",
    "\tframe_features = get_feature_vector(frame, model, mean_std)\n",
    "\tframe_features_list.append(frame_features)\n",
    "\t# print(f\"{i} frames.\")\n",
    "\n",
    "frame_features_list_squeezed = [np.squeeze(f) for f in frame_features_list]\n",
    "\n",
    "# 11 min\n",
    "# change my normalization technique using this MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar frame for panel mta_data/panels/fma/panel_4.png, mta_data/frames/fma/Pframes_greys/fma_04_i_frame_8888.jpg (similarity: 0.58)\n"
     ]
    }
   ],
   "source": [
    "# Read the manga panel image\n",
    "panel_path = 'mta_data/panels/fma/panel_4.png'\n",
    "panel = cv2.imread(panel_path)\n",
    "panel = transform(panel).unsqueeze(0)\n",
    "panel_features = get_feature_vector(panel, model, mean_std)\n",
    "\n",
    "# Initialize the most similar frame and its similarity score\n",
    "most_similar_frame = None\n",
    "highest_similarity = -1\n",
    "\n",
    "# Compare the manga panel with all anime frames\n",
    "for i, frame_features in enumerate(frame_features_list):\n",
    "\tsimilarity = calculate_similarity(panel_features, frame_features)\n",
    "\tif similarity > highest_similarity:\n",
    "\t\thighest_similarity = similarity\n",
    "\t\tmost_similar_frame = frame_paths[i]\n",
    "\n",
    "# Print the most similar frame and its similarity score\n",
    "print(f\"Most similar frame for panel {panel_path}, {most_similar_frame} (similarity: {highest_similarity:.2f})\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most similar frame for panel mta_data/panels/fma/panel_0.png, mta_data/frames/fma/Pframes_greys/fma_04_i_frame_8800.jpg (similarity: 0.63), imagenet\n",
    "Most similar frame for panel mta_data/panels/fma/panel_0.png, mta_data/frames/fma/Pframes_greys/fma_04_i_frame_8923.jpg (similarity: 0.54)\n",
    "Most similar frame for panel mta_data/panels/fma/panel_4.png, mta_data/frames/fma/Pframes_greys/fma_04_i_frame_8888.jpg (similarity: 0.58), imagenet\n",
    "Most similar frame for panel mta_data/panels/fma/panel_4.png, mta_data/frames/fma/Pframes_greys/fma_04_i_frame_8888.jpg (similarity: 0.58)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "826286f252ddcff787408661d182f2c63da5c69a1bdcb0cb2e03ac788e5f338e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
