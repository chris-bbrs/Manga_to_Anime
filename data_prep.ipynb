{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import imageio.v3 as iio\n",
    "# from pymediainfo import MediaInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "ep_path = f\"data/{anime}/episodes/\"\n",
    "abs_ep_path = os.path.abspath(ep_path)\n",
    "episodes = [os.path.join(ep_path, file) for file in os.listdir(ep_path)][2]\n",
    "\n",
    "# media_info = MediaInfo.parse(episodes[0])\n",
    "# duration_sec = media_info.tracks[0].duration / 1000\n",
    "# frame_rate = media_info.video_tracks[0].frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/Fullmetal Alchemist BrotherHood 04.mkv'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = \"data/Fullmetal Alchemist BrotherHood 04.mkv\"\n",
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "\n",
    "# Iframes and Pframes\n",
    "filename = episodes\n",
    "save_path = f\"data/{anime}/frames/ep4_unique_test\"\n",
    "\n",
    "def get_frame_types(video_fn):\n",
    "\tcommand = 'ffprobe -v error -show_entries frame=pict_type -of default=noprint_wrappers=1'.split()\n",
    "\tout = subprocess.check_output(command + [video_fn]).decode()\n",
    "\tframe_types = out.replace('pict_type=','').split()\n",
    "\treturn zip(range(len(frame_types)), frame_types)\n",
    "\n",
    "def save_keyframes(video_fn, out_path):\n",
    "\tframe_types = get_frame_types(video_fn)\n",
    "\ti_frames = [x[0] for x in frame_types if x[1]=='P']\n",
    "\tif i_frames:\n",
    "\t\tbasename = os.path.splitext(os.path.basename(video_fn))[0]\n",
    "\t\tcap = cv2.VideoCapture(video_fn)\n",
    "\t\tfor frame_no in i_frames:\n",
    "\t\t\tcap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "\t\t\tret, frame = cap.read()\n",
    "\t\t\toutname = os.path.join(out_path, f\"{basename}_i_frame_{str(frame_no)}.jpg\")\n",
    "\t\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\tcv2.imwrite(outname, frame)\n",
    "\t\tcap.release()\n",
    "\telse:\n",
    "\t\tprint ('No I-frames in '+video_fn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tsave_keyframes(filename, save_path)\n",
    "# Pframes: 8m, Iframes: 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "\n",
    "content = episodes\n",
    "with av.open(content) as container:\n",
    "\t# Signal that we only want to look at keyframes.\n",
    "\tstream = container.streams.video[0]\n",
    "\tstream.codec_context.skip_frame = \"NONREF\"\n",
    "\tstream.codec_context.thread_type = \"FRAME\"\n",
    "\n",
    "\tfor frame in container.decode(stream):\n",
    "\n",
    "\t\t# We use `frame.pts` as `frame.index` won't make must sense with the `skip_frame`.\n",
    "\t\tsave_path = os.path.join(f\"mta_data/{anime}/frames/ep3\", f\"fma_key_frame_{frame.pts}.jpg\")\n",
    "\t\t# frame.to_image().save(save_path)\n",
    "\t\t\n",
    "\t\t # Convert the frame to grayscale.\n",
    "\t\tgray_frame = frame.to_image().convert(\"L\")\n",
    "\t\tgray_frame.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "def with_ztransform_preprocess(hashfunc, hash_size=8):\n",
    "\tdef function(path):\n",
    "\t\timage = Image.open(path)\n",
    "\t\timage = image.convert(\"L\").resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "\t\tdata = image.getdata()\n",
    "\t\tquantiles = np.arange(100)\n",
    "\t\tquantiles_values = np.percentile(data, quantiles)\n",
    "\t\tzdata = (np.interp(data, quantiles_values, quantiles) / 100 * 255).astype(np.uint8)\n",
    "\t\timage.putdata(zdata)\n",
    "\t\treturn hashfunc(image)\n",
    "\treturn function\n",
    "\n",
    "dhash_z_transformed = with_ztransform_preprocess(imagehash.dhash, hash_size = 8)\n",
    "out_folder = f\"mta_data/frames/{anime}/Pframes_filtered\"\n",
    "pics = [os.path.join(out_folder, file) for file in os.listdir(out_folder)]\n",
    "hashes = [dhash_z_transformed(pic) for pic in pics]\n",
    "\n",
    "df = pd.DataFrame({\"image_ids\": pics, \"hash_values\": hashes, })\n",
    "df_clean = df[df.duplicated(['hash_values'], keep=False)]\n",
    "for image in df_clean[\"image_ids\"].values:\n",
    "\tos.remove(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 19:44:41.085167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-17 19:44:44.558391: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-17 19:44:58.481689: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/c/Users/chris/Documents/projects/manga_to_anime/.venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-17 19:44:58.485133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /mnt/c/Users/chris/Documents/projects/manga_to_anime/.venv/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-17 19:44:58.485309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def remove_text(model_path, original_img, debug_display=False):\n",
    "\ttf.data.experimental.enable_debug_mode()\n",
    "\t# Load the model\n",
    "\tmodel = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "\t# Pre-process the image\n",
    "\theight, width = original_img.shape[:2]\n",
    "\n",
    "\timg = np.expand_dims(original_img, axis=-1)  # add an extra dimension for the channel\n",
    "\timg = np.repeat(img, 3, axis=-1)  # repeat the image along the channel dimension\n",
    "\timg = resize(img, (768,512), anti_aliasing=True, preserve_range=True)\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\timg = img/255\n",
    "\n",
    "\t# Use the model to make a prediction\n",
    "\tp = model.predict(img)\n",
    "\n",
    "\tmask = p[0,:,:,0]\n",
    "\n",
    "\tmask = np.round(mask, 0)\n",
    "\n",
    "\t# Return the image to its original dimensions\n",
    "\tmask_img = Image.fromarray(mask)\n",
    "\tmask_img = mask_img.resize((width,height), resample=Image.Resampling.BICUBIC)\n",
    "\tmask = np.asarray(mask_img)\n",
    "\t\n",
    "\t# Convert the mask to a boolean array\n",
    "\tmask = mask == 1\n",
    "\n",
    "\tno_text = original_img.copy()\n",
    "\t# Set the pixels in the original image to 1 wherever the mask is 0\n",
    "\tno_text[mask] = 255\n",
    "\t# no_text = np.where(mask, mask, no_text)\n",
    "\n",
    "\t# Increase the contrast of the image\n",
    "\tno_text = exposure.rescale_intensity(no_text, out_range=(0, 255))\n",
    "\tno_text = cv2.normalize(src=no_text, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "\tif debug_display == True:\n",
    "\t\t# Display the resulting image\n",
    "\t\tplt.imshow(mask, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\t\tplt.imshow(original_img, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\t\tplt.imshow(no_text, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\telse:\n",
    "\t\treturn no_text\n",
    "\t\t\n",
    "# img_path = test[3]\n",
    "# remove_text(model_path=\"0207_e500_std_model_4.h5\", img_path=img_path, debug_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def pil_resize(img):\n",
    "\t# Get the original aspect ratio\n",
    "\toriginal_aspect = img.width / img.height\n",
    "\n",
    "\t# Determine the new width and height\n",
    "\tnew_width = 256\n",
    "\tnew_height = int(new_width / original_aspect)\n",
    "\n",
    "\t# Resize the image\n",
    "\tresized_im = img.resize((new_width, new_height), resample=Image.Resampling.BICUBIC)\n",
    "\n",
    "\t# Check if the image is larger than 256x256\n",
    "\tif img.width > 256 or img.height > 256:\n",
    "\t\t# Resize the image using thumbnail method\n",
    "\t\tresized_im.thumbnail((256, 256), resample=Image.Resampling.BICUBIC)\n",
    "\n",
    "\t# Create a new image with white background (255 represents white)\n",
    "\tnew_im = Image.new(\"L\", (256, 256), 255)\n",
    "\n",
    "\t# Paste the resized image on center of the new image\n",
    "\tleft = (256 - resized_im.width) // 2\n",
    "\ttop = (256 - resized_im.height) // 2\n",
    "\tnew_im.paste(resized_im, (left, top))\n",
    "\t\n",
    "\treturn new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kumiko import kumikolib\n",
    "import re\n",
    "\n",
    "def enough_info(image, thresh=95):\n",
    "\timage = cv2.equalizeHist(image)\n",
    "\t_, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\ttotal_pix = image.shape[0] * image.shape[1]\n",
    "\twhite = np.sum(image >= 254)\n",
    "\twhite_percent = round((white / total_pix) * 100)\n",
    "\tblack = np.sum(image <= 1)\n",
    "\tblack_percent = round((black / total_pix) * 100)\n",
    "\t# print(black_percent, white_percent)\n",
    "\tif (white_percent < thresh and black_percent < thresh):\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\n",
    "def panel_extractor(im_path, save_path):\n",
    "\tk = kumikolib.Kumiko()\n",
    "\t# Open the input image\n",
    "\t# img = cv2.imread(im_path, cv2.IMREAD_GRAYSCALE)\n",
    "\timg = Image.open(im_path)\n",
    "\t# Check the mode of the image\n",
    "\tif img.mode == 'RGB':\n",
    "\t\treturn\n",
    "\timg = img.convert('L')\n",
    "\timg = np.array(img)\n",
    "\t# img = remove_text(model_path=\"0207_e500_std_model_4.h5\", original_img=img)\n",
    "\tim_size = img.shape[0] * img.shape[1]\n",
    "\n",
    "\tif enough_info(img, thresh=95):\n",
    "\t\tinfo = k.parse_image(im_path, image=img)\n",
    "\t\tpanels = info[\"panels\"]\n",
    "\n",
    "\t\tpanel_percent = 0.05\n",
    "\t\tfor index, panel in enumerate(panels):\n",
    "\t\t\tcrop_img = img[panel[1]:panel[1]+panel[3], panel[0]:panel[0]+panel[2]]\n",
    "\t\t\tif (crop_img.shape[0] * crop_img.shape[1] > panel_percent * im_size):\n",
    "\t\t\t\t# if enough_info(crop_img, thresh=95):\n",
    "\t\t\t\toutput_path = os.path.join(save_path, 'panel_{}-{}.png'.format(re.search(r'\\D*(\\d+)\\.\\D*', im_path).group(1), index))\n",
    "\t\t\t\tcv2.imwrite(output_path, crop_img)\n",
    "\t\t\t\t# pil_img = Image.fromarray(crop_img)\n",
    "\t\t\t\t# res = pil_resize(pil_img)\n",
    "\t\t\t\t# res.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "anime = 'fma'\n",
    "manga_path = f\"data/{anime}/manga\"\n",
    "panel_path = f\"data/{anime}/panels\"\n",
    "manga_chapters = natsorted(os.listdir(manga_path))\n",
    "manga = {}\n",
    "for chapter in manga_chapters:\n",
    "\tch_path = os.path.join(manga_path, chapter)\n",
    "\tmanga[chapter] = natsorted([os.path.join(ch_path, file) for file in os.listdir(ch_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f\"data\"\n",
    "chapter = \"ch5\"\n",
    "save_path = os.path.join(panel_path, chapter)\n",
    "for mng in manga[chapter]:\n",
    "\tpanel_extractor(mng, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/x6.png\n"
     ]
    }
   ],
   "source": [
    "save_path = f\"data\"\n",
    "im_path = \"data/x6.png\"\n",
    "\n",
    "print(im_path)\n",
    "panel_extractor(im_path, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"data/panel_4-0.png\",\n",
    "\"data/panel_6-0.png\",\n",
    "\"data/panel_7-0.png\",\n",
    "\"data/panel_7-2.png\",\n",
    "\"data/panel_10-2.png\",\n",
    "\"data/panel_23-1.png\",\n",
    "\"data/panel_42-3.png\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### panel-anime correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "frame_path = f\"data/{anime}/frames/ep4_unique_test\"\n",
    "anime_frames = [os.path.join(frame_path, file) for file in os.listdir(frame_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def with_ztransform_preprocess(hashfunc, image, hash_size=8):\n",
    "\timage = image.convert(\"L\").resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "\tdata = image.getdata()\n",
    "\tquantiles = np.arange(100)\n",
    "\tquantiles_values = np.percentile(data, quantiles)\n",
    "\tzdata = (np.interp(data, quantiles_values, quantiles) / 100 * 255).astype(np.uint8)\n",
    "\timage.putdata(zdata)\n",
    "\n",
    "\thash = hashfunc(image)\n",
    "\n",
    "\treturn hash\n",
    "\n",
    "def compare_panel_to_frames(hashfunc, panel, frame_hashes, anime_frames, hash_size=8):\n",
    "\tpanel_hash = with_ztransform_preprocess(hashfunc, panel, hash_size)\n",
    "\n",
    "\tpanel_distances = []\n",
    "\t# Compare the manga panel to each anime frame\n",
    "\tfor frame_hash in frame_hashes:\n",
    "\t\tdistance = panel_hash - frame_hash\n",
    "\t\tpanel_distances.append(distance)\n",
    "\t# Find the index of the minimum distance for the current manga panel\n",
    "\tmin_index = panel_distances.index(min(panel_distances))\n",
    "\t# The min_index will be the index of the anime frame that is most similar to the current manga panel\n",
    "\tmost_similar_frame = anime_frames[min_index]\n",
    "\t# Return the most similar frame and the minimum distance\n",
    "\treturn (most_similar_frame, min(panel_distances))\n",
    "\n",
    "panels = ['mta_data/panels/fma/panel_4.jpg']\n",
    "\n",
    "# Calculate the hash function for the anime frames using pHash\n",
    "hashfunc = imagehash.phash\n",
    "\n",
    "# Calculate the hashes of the anime frames\n",
    "frame_hashes = []\n",
    "for frame_path in anime_frames:\n",
    "\tframe = Image.open(frame_path)\n",
    "\tframe_hash = with_ztransform_preprocess(hashfunc, frame)\n",
    "\tframe_hashes.append(frame_hash)\n",
    "\n",
    "# Initialize an empty list to store the distances\n",
    "distances = []\n",
    "\n",
    "# Iterate over the manga panels\n",
    "for panel_path in panels:\n",
    "\tpanel = Image.open(panel_path)\n",
    "\t# Compare the manga panel to the anime frames\n",
    "\tresult = compare_panel_to_frames(hashfunc, panel, frame_hashes, anime_frames)\n",
    "\t# Append the result to the distances list\n",
    "\tdistances.append(result)\n",
    "\n",
    "img = mpimg.imread(distances[0][0])\n",
    "imgplot = plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3251) tensor(0.1875)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_mean_std(image_paths, my_batch_size=32):\n",
    "\t# Define a dataset that reads the images one at a time\n",
    "\tclass ImageDataset(torch.utils.data.Dataset):\n",
    "\t\tdef __init__(self, image_paths, transform=None):\n",
    "\t\t\tself.image_paths = image_paths\n",
    "\t\t\tself.transform = transform\n",
    "\n",
    "\t\tdef __getitem__(self, index):\n",
    "\t\t\timage = Image.open(self.image_paths[index])\n",
    "\t\t\tif self.transform:\n",
    "\t\t\t\timage = self.transform(image)\n",
    "\t\t\treturn image\n",
    "\n",
    "\t\tdef __len__(self):\n",
    "\t\t\treturn len(self.image_paths)\n",
    "\n",
    "\t# Define a transform that converts the images to tensors\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t])\n",
    "\n",
    "\t# Create a dataset and a dataloader\n",
    "\tdataset = ImageDataset(image_paths, transform=transform)\n",
    "\tdataloader = DataLoader(dataset, batch_size=my_batch_size, num_workers=4)\n",
    "\n",
    "\t# Calculate mean and std\n",
    "\tmean = 0.0\n",
    "\tstd = 0.0\n",
    "\tcount = 0\n",
    "\tfor images in dataloader:\n",
    "\t\timages = images.view(images.size(0), -1)\n",
    "\t\tmean += images.mean(dim=1).sum()\n",
    "\t\tstd += images.std(dim=1).sum()\n",
    "\t\tcount += images.size(0)\n",
    "\tmean /= count\n",
    "\tstd /= count\n",
    "\n",
    "\treturn mean, std\n",
    "\n",
    "image_paths = anime_frames\n",
    "mean, std = calculate_mean_std(image_paths)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spatial\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the mean and standard deviation values for ImageNet\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# Define the mean and standard deviation values from my images\n",
    "mean_std = (mean, std)\n",
    "\n",
    "# Create the normalization transform\n",
    "transform = transforms.Compose([\n",
    "\t# transforms.Grayscale(),\n",
    "\ttransforms.Resize((224, 224)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=mean, std=std),\n",
    "\ttransforms.Lambda(lambda x: x.expand(1, 3, -1, -1).view(1, 3, 224, 224))\n",
    "])\n",
    "\n",
    "# Load the ResNet50 model from PyTorch's model zoo\n",
    "model = torchvision.models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Extract the features from the model's second-to-last fully-connected layer\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Read the anime frame images\n",
    "frame_paths = anime_frames\n",
    "\n",
    "# Extract the features for all anime frames\n",
    "frame_features_list = []\n",
    "for i, frame_path in enumerate(frame_paths):\n",
    "\tframe = Image.open(frame_path)\n",
    "\timg_tensor = transform(frame)\n",
    "\tframe_features = model(img_tensor).detach().numpy()\n",
    "\tframe_features_list.append(frame_features)\n",
    "\t# print(f\"{i} frames.\")\n",
    "\n",
    "frame_features_list_squeezed = [np.squeeze(f) for f in frame_features_list]\n",
    "\n",
    "# 11 min\n",
    "# another normalization technique using this MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_paths[0] == 'data/fma/frames/ep4_unique_test/Fullmetal Alchemist BrotherHood 04_i_frame_1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(vector1, vector2):\n",
    "\t# Convert the vectors to PyTorch tensors\n",
    "\tvector1 = torch.tensor(vector1)\n",
    "\tvector2 = torch.tensor(vector2)\n",
    "\t\n",
    "\t# Flatten the tensors to 1D\n",
    "\tvector1 = vector1.view(-1)\n",
    "\tvector2 = vector2.view(-1)\n",
    "\t\n",
    "\t# Calculate the dot product and norms of the two feature vectors\n",
    "\tdot_product = torch.dot(vector1, vector2)\n",
    "\tnorm1 = torch.norm(vector1)\n",
    "\tnorm2 = torch.norm(vector2)\n",
    "\t\n",
    "\t# Calculate the cosine similarity between the two feature vectors\n",
    "\tsimilarity = dot_product / (norm1 * norm2)\n",
    "\treturn similarity.tolist()\n",
    "\n",
    "# Read the manga panel image\n",
    "panel_path = 'data/panel_6-3.png'\n",
    "panel = Image.open(panel_path)\n",
    "panel = transform(panel)\n",
    "panel_features = model(panel).detach().numpy()\n",
    "\n",
    "# Initialize the most similar frame and its similarity score\n",
    "most_similar_frame = None\n",
    "highest_similarity = -1\n",
    "\n",
    "# Compare the manga panel with all anime frames\n",
    "for i, frame_features in enumerate(frame_features_list):\n",
    "\tsimilarity = calculate_similarity(panel_features, frame_features)\n",
    "\tif similarity > highest_similarity:\n",
    "\t\thighest_similarity = similarity\n",
    "\t\tmost_similar_frame = frame_paths[i]\n",
    "\n",
    "# Print the most similar frame and its similarity score\n",
    "print(f\"Most similar frame for panel {panel_path}, {most_similar_frame} (similarity: {highest_similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_path = f\"mta_data/frames/{anime}/Pframes_resnet\"\n",
    "frame_paths = [os.path.join(frame_path, file) for file in os.listdir(frame_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trees 14\n",
      "log num_neighbors 443\n"
     ]
    }
   ],
   "source": [
    "import annoy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calculate_similarity(vector1, vector2):\n",
    "\t# Convert the vectors to PyTorch tensors\n",
    "\tvector1 = torch.tensor(vector1)\n",
    "\tvector2 = torch.tensor(vector2)\n",
    "\t\n",
    "\t# Flatten the tensors to 1D\n",
    "\tvector1 = vector1.view(-1)\n",
    "\tvector2 = vector2.view(-1)\n",
    "\t\n",
    "\t# Calculate the dot product and norms of the two feature vectors\n",
    "\tdot_product = torch.dot(vector1, vector2)\n",
    "\tnorm1 = torch.norm(vector1)\n",
    "\tnorm2 = torch.norm(vector2)\n",
    "\t\n",
    "\t# Calculate the cosine similarity between the two feature vectors\n",
    "\tsimilarity = dot_product / (norm1 * norm2)\n",
    "\treturn similarity.tolist()\n",
    "\n",
    "def calculate_optimal_trees(n_samples, accuracy):\n",
    "\t# Calculate the optimal number of trees based on the size of the dataset and the desired accuracy\n",
    "\tn_trees = int(np.round(np.log(n_samples) / np.log(2) / accuracy))\n",
    "\treturn n_trees\n",
    "\n",
    "# Build the Annoy index from the frame feature vectors\n",
    "annoy_index = annoy.AnnoyIndex(len(frame_features_list_squeezed[0]), metric='angular')\n",
    "for i, frame_features in enumerate(frame_features_list_squeezed):\n",
    "\tannoy_index.add_item(i, frame_features)\n",
    "\n",
    "# Calculate the sample length of the dataset\n",
    "n_samples = len(frame_features_list_squeezed)\n",
    "\n",
    "# Set the desired accuracy of the search\n",
    "accuracy = 0.9\n",
    "# Set the similarity threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Calculate the optimal number of trees for a dataset with n_samples and an accuracy of accuracy\n",
    "n_trees = calculate_optimal_trees(n_samples, accuracy)\n",
    "print(\"n_trees\", n_trees)\n",
    "annoy_index.build(n_trees)\n",
    "\n",
    "# Set the number of nearest neighbors to search for\n",
    "num_neighbors = int(n_samples * math.log(1 / accuracy) * 0.5)\n",
    "print(\"log num_neighbors\", num_neighbors)\n",
    "\n",
    "# Create a list to store the indexes of the frames to delete\n",
    "frames_to_delete = []\n",
    "\n",
    "# Iterate through the list of frame feature vectors\n",
    "for i, frame_features in enumerate(frame_features_list_squeezed):\n",
    "\t# Check if the current frame has already been added to the list of frames to delete\n",
    "\tif i in frames_to_delete:\n",
    "\t\tcontinue\n",
    "\t# Find the nearest neighbors of the current frame in the Annoy index\n",
    "\tindices = annoy_index.get_nns_by_item(i, num_neighbors)\n",
    "\t# Iterate through the nearest neighbors\n",
    "\tfor j in indices:\n",
    "\t\t# Skip the current frame\n",
    "\t\tif i == j:\n",
    "\t\t\tcontinue\n",
    "\t\t# Calculate the similarity between the current frame and the next nearest neighbor\n",
    "\t\tsimilarity = calculate_similarity(frame_features, frame_features_list_squeezed[j])\n",
    "\t\t# If the similarity is above the threshold, add the nearest neighbor to the list of frames to delete\n",
    "\t\tif similarity > threshold:\n",
    "\t\t\tframes_to_delete.append(j)\n",
    "\n",
    "# Create a list to store the paths of the images to delete\n",
    "image_paths_to_delete = [frame_paths[i] for i in range(len(frame_paths)) if i in frames_to_delete]\n",
    "\n",
    "# Create a ThreadPoolExecutor with a fixed number of threads\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "\t # Submit the delete tasks to the executor\n",
    "\t for image_path in image_paths_to_delete:\n",
    "\t\t executor.submit(os.remove, image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "826286f252ddcff787408661d182f2c63da5c69a1bdcb0cb2e03ac788e5f338e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
