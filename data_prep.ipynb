{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import imageio.v3 as iio\n",
    "# from pymediainfo import MediaInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "ep_path = f\"data/{anime}/episodes/\"\n",
    "abs_ep_path = os.path.abspath(ep_path)\n",
    "episodes = [os.path.join(ep_path, file) for file in os.listdir(ep_path)][1]\n",
    "\n",
    "# media_info = MediaInfo.parse(episodes[0])\n",
    "# duration_sec = media_info.tracks[0].duration / 1000\n",
    "# frame_rate = media_info.video_tracks[0].frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mta_data/fma/episodes/Fullmetal Alchemist BrotherHood 03.mkv'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import cv2\n",
    "\n",
    "# Iframes and Pframes\n",
    "filename = episodes\n",
    "\n",
    "def get_frame_types(video_fn):\n",
    "\tcommand = 'ffprobe -v error -show_entries frame=pict_type -of default=noprint_wrappers=1'.split()\n",
    "\tout = subprocess.check_output(command + [video_fn]).decode()\n",
    "\tframe_types = out.replace('pict_type=','').split()\n",
    "\treturn zip(range(len(frame_types)), frame_types)\n",
    "\n",
    "def save_i_keyframes(video_fn):\n",
    "\tframe_types = get_frame_types(video_fn)\n",
    "\ti_frames = [x[0] for x in frame_types if x[1]=='P']\n",
    "\tif i_frames:\n",
    "\t\tbasename = os.path.splitext(os.path.basename(video_fn))[0]\n",
    "\t\tcap = cv2.VideoCapture(video_fn)\n",
    "\t\tfor frame_no in i_frames:\n",
    "\t\t\tcap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
    "\t\t\tret, frame = cap.read()\n",
    "\t\t\toutname = os.path.join(f\"mta_data/{anime}/frames/Pframes_greys\", f\"{basename}_i_frame_{str(frame_no)}.jpg\")\n",
    "\t\t\tframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\t\tcv2.imwrite(outname, frame)\n",
    "\t\tcap.release()\n",
    "\telse:\n",
    "\t\tprint ('No I-frames in '+video_fn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tsave_i_keyframes(filename)\n",
    "# Pframes: 8m, Iframes: 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "\n",
    "content = episodes\n",
    "with av.open(content) as container:\n",
    "\t# Signal that we only want to look at keyframes.\n",
    "\tstream = container.streams.video[0]\n",
    "\tstream.codec_context.skip_frame = \"NONREF\"\n",
    "\tstream.codec_context.thread_type = \"FRAME\"\n",
    "\n",
    "\tfor frame in container.decode(stream):\n",
    "\n",
    "\t\t# We use `frame.pts` as `frame.index` won't make must sense with the `skip_frame`.\n",
    "\t\tsave_path = os.path.join(f\"mta_data/{anime}/frames/ep3\", f\"fma_key_frame_{frame.pts}.jpg\")\n",
    "\t\t# frame.to_image().save(save_path)\n",
    "\t\t\n",
    "\t\t # Convert the frame to grayscale.\n",
    "\t\tgray_frame = frame.to_image().convert(\"L\")\n",
    "\t\tgray_frame.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "def with_ztransform_preprocess(hashfunc, hash_size=8):\n",
    "\tdef function(path):\n",
    "\t\timage = Image.open(path)\n",
    "\t\timage = image.convert(\"L\").resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "\t\tdata = image.getdata()\n",
    "\t\tquantiles = np.arange(100)\n",
    "\t\tquantiles_values = np.percentile(data, quantiles)\n",
    "\t\tzdata = (np.interp(data, quantiles_values, quantiles) / 100 * 255).astype(np.uint8)\n",
    "\t\timage.putdata(zdata)\n",
    "\t\treturn hashfunc(image)\n",
    "\treturn function\n",
    "\n",
    "dhash_z_transformed = with_ztransform_preprocess(imagehash.dhash, hash_size = 8)\n",
    "out_folder = f\"mta_data/frames/{anime}/Pframes_filtered\"\n",
    "pics = [os.path.join(out_folder, file) for file in os.listdir(out_folder)]\n",
    "hashes = [dhash_z_transformed(pic) for pic in pics]\n",
    "\n",
    "df = pd.DataFrame({\"image_ids\": pics, \"hash_values\": hashes, })\n",
    "df_clean = df[df.duplicated(['hash_values'], keep=False)]\n",
    "for image in df_clean[\"image_ids\"].values:\n",
    "\tos.remove(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "\n",
    "anime = 'fma'\n",
    "manga_path = f\"data/{anime}/manga/ch5\"\n",
    "manga = natsorted([os.path.join(manga_path, file) for file in os.listdir(manga_path)])\n",
    "test = manga[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "\n",
    "def remove_text(model_path, original_img, debug_display=False):\n",
    "\ttf.data.experimental.enable_debug_mode()\n",
    "\t# Load the model\n",
    "\tmodel = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "\t# Pre-process the image\n",
    "\theight, width = original_img.shape[:2]\n",
    "\n",
    "\timg = np.expand_dims(original_img, axis=-1)  # add an extra dimension for the channel\n",
    "\timg = np.repeat(img, 3, axis=-1)  # repeat the image along the channel dimension\n",
    "\timg = resize(img, (768,512), anti_aliasing=True, preserve_range=True)\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\timg = img/255\n",
    "\n",
    "\t# Use the model to make a prediction\n",
    "\tp = model.predict(img)\n",
    "\n",
    "\tmask = p[0,:,:,0]\n",
    "\n",
    "\tmask = np.round(mask, 0)\n",
    "\n",
    "\t# Return the image to its original dimensions\n",
    "\tmask_img = Image.fromarray(mask)\n",
    "\tmask_img = mask_img.resize((width,height), resample=Image.Resampling.BICUBIC)\n",
    "\tmask = np.asarray(mask_img)\n",
    "\t\n",
    "\t# Convert the mask to a boolean array\n",
    "\tmask = mask == 1\n",
    "\n",
    "\tno_text = original_img.copy()\n",
    "\t# Set the pixels in the original image to 1 wherever the mask is 0\n",
    "\tno_text[mask] = 255\n",
    "\t# no_text = np.where(mask, mask, no_text)\n",
    "\n",
    "\t# Increase the contrast of the image\n",
    "\tno_text = exposure.rescale_intensity(no_text, out_range=(0, 255))\n",
    "\tno_text = cv2.normalize(src=no_text, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "\tif debug_display == True:\n",
    "\t\t# Display the resulting image\n",
    "\t\tplt.imshow(mask, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\t\tplt.imshow(original_img, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\t\tplt.imshow(no_text, cmap='gray')\n",
    "\t\tplt.show()\n",
    "\telse:\n",
    "\t\treturn no_text\n",
    "\t\t\n",
    "# img_path = test[3]\n",
    "# remove_text(model_path=\"0207_e500_std_model_4.h5\", img_path=img_path, debug_display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kumiko import kumikolib\n",
    "\n",
    "def enough_info(image, thresh=95):\n",
    "\timage = cv2.equalizeHist(image)\n",
    "\t_, image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\ttotal_pix = image.shape[0] * image.shape[1]\n",
    "\twhite = np.sum(image >= 254)\n",
    "\twhite_percent = (white / total_pix) * 100\n",
    "\tblack = np.sum(image <= 1)\n",
    "\tblack_percent = (black / total_pix) * 100\n",
    "\tprint(black_percent, white_percent)\n",
    "\tif (white_percent < thresh and black_percent < thresh):\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\n",
    "def panel_extractor(im_path, save_path):\n",
    "\tk = kumikolib.Kumiko()\n",
    "\t# img = cv2.imread(im_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\t# Open the input image\n",
    "\timg = Image.open(im_path)\n",
    "\t# Check the mode of the image\n",
    "\tif img.mode == 'RGB':\n",
    "\t\tprint(\"RGB\")\n",
    "\t\treturn\n",
    "\timg = img.convert('L')\n",
    "\timg = np.array(img)\n",
    "\timg = remove_text(model_path=\"0207_e500_std_model_4.h5\", original_img=img)\n",
    "\n",
    "\tif enough_info(img, thresh=95):\n",
    "\t\tinfo = k.parse_image(im_path, image=img)\n",
    "\t\tpanels = info[\"panels\"]\n",
    "\n",
    "\t\tfor index, panel in enumerate(panels):\n",
    "\t\t\tcrop_img = img[panel[1]:panel[1]+panel[3], panel[0]:panel[0]+panel[2]]\n",
    "\t\t\toutput_path = os.path.join(save_path, f'panel_{index}.png')\n",
    "\t\t\tcv2.imwrite(output_path, crop_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/fma/manga/ch1/x5.png\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "97.16926095487246 2.830739045127534\n"
     ]
    }
   ],
   "source": [
    "save_path = f\"data\"\n",
    "im_path = test[1]\n",
    "\n",
    "print(im_path)\n",
    "panel_extractor(im_path, save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### panel-anime correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime = 'fma'\n",
    "frame_path = f\"mta_data/frames/{anime}/Pframes_greys\"\n",
    "anime_frames = [os.path.join(frame_path, file) for file in os.listdir(frame_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def with_ztransform_preprocess(hashfunc, image, hash_size=8):\n",
    "\timage = image.convert(\"L\").resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
    "\tdata = image.getdata()\n",
    "\tquantiles = np.arange(100)\n",
    "\tquantiles_values = np.percentile(data, quantiles)\n",
    "\tzdata = (np.interp(data, quantiles_values, quantiles) / 100 * 255).astype(np.uint8)\n",
    "\timage.putdata(zdata)\n",
    "\n",
    "\thash = hashfunc(image)\n",
    "\n",
    "\treturn hash\n",
    "\n",
    "def compare_panel_to_frames(hashfunc, panel, frame_hashes, anime_frames, hash_size=8):\n",
    "\tpanel_hash = with_ztransform_preprocess(hashfunc, panel, hash_size)\n",
    "\n",
    "\tpanel_distances = []\n",
    "\t# Compare the manga panel to each anime frame\n",
    "\tfor frame_hash in frame_hashes:\n",
    "\t\tdistance = panel_hash - frame_hash\n",
    "\t\tpanel_distances.append(distance)\n",
    "\t# Find the index of the minimum distance for the current manga panel\n",
    "\tmin_index = panel_distances.index(min(panel_distances))\n",
    "\t# The min_index will be the index of the anime frame that is most similar to the current manga panel\n",
    "\tmost_similar_frame = anime_frames[min_index]\n",
    "\t# Return the most similar frame and the minimum distance\n",
    "\treturn (most_similar_frame, min(panel_distances))\n",
    "\n",
    "panels = ['mta_data/panels/fma/panel_4.jpg']\n",
    "\n",
    "# Calculate the hash function for the anime frames using pHash\n",
    "hashfunc = imagehash.phash\n",
    "\n",
    "# Calculate the hashes of the anime frames\n",
    "frame_hashes = []\n",
    "for frame_path in anime_frames:\n",
    "\tframe = Image.open(frame_path)\n",
    "\tframe_hash = with_ztransform_preprocess(hashfunc, frame)\n",
    "\tframe_hashes.append(frame_hash)\n",
    "\n",
    "# Initialize an empty list to store the distances\n",
    "distances = []\n",
    "\n",
    "# Iterate over the manga panels\n",
    "for panel_path in panels:\n",
    "\tpanel = Image.open(panel_path)\n",
    "\t# Compare the manga panel to the anime frames\n",
    "\tresult = compare_panel_to_frames(hashfunc, panel, frame_hashes, anime_frames)\n",
    "\t# Append the result to the distances list\n",
    "\tdistances.append(result)\n",
    "\n",
    "img = mpimg.imread(distances[0][0])\n",
    "imgplot = plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_frame = cv2.imread(anime_frame_path)\n",
    "frame_height, frame_width, _ = anime_frame.shape\n",
    "manga_panel_gray = cv2.resize(manga_panel_gray, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def calculate_mean_std(image_paths, my_batch_size=32):\n",
    "\t# Define a dataset that reads the images one at a time\n",
    "\tclass ImageDataset(torch.utils.data.Dataset):\n",
    "\t\tdef __init__(self, image_paths, transform=None):\n",
    "\t\t\tself.image_paths = image_paths\n",
    "\t\t\tself.transform = transform\n",
    "\n",
    "\t\tdef __getitem__(self, index):\n",
    "\t\t\timage = Image.open(self.image_paths[index])\n",
    "\t\t\tif self.transform:\n",
    "\t\t\t\timage = self.transform(image)\n",
    "\t\t\treturn image\n",
    "\n",
    "\t\tdef __len__(self):\n",
    "\t\t\treturn len(self.image_paths)\n",
    "\n",
    "\t# Define a transform that converts the images to tensors\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t])\n",
    "\n",
    "\t# Create a dataset and a dataloader\n",
    "\tdataset = ImageDataset(image_paths, transform=transform)\n",
    "\tdataloader = DataLoader(dataset, batch_size=my_batch_size, num_workers=4)\n",
    "\n",
    "\t# Calculate mean and std\n",
    "\tmean = 0.0\n",
    "\tstd = 0.0\n",
    "\tcount = 0\n",
    "\tfor images in dataloader:\n",
    "\t\timages = images.view(images.size(0), -1)\n",
    "\t\tmean += images.mean(dim=1).sum()\n",
    "\t\tstd += images.std(dim=1).sum()\n",
    "\t\tcount += images.size(0)\n",
    "\tmean /= count\n",
    "\tstd /= count\n",
    "\n",
    "\treturn mean, std\n",
    "\n",
    "image_paths = anime_frames\n",
    "mean, std = calculate_mean_std(image_paths)\n",
    "# mean of pframes_grey = 0.3965, std = 0.1852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import spatial\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def resize(img, size):\n",
    "\t# Check if the image is a tensor\n",
    "\tif isinstance(img, torch.Tensor):\n",
    "\t\t# Remove the extra dimensions if the image is 4D\n",
    "\t\tif img.ndim == 4:\n",
    "\t\t\timg = img.squeeze(0)\n",
    "\t\t# Convert the tensor to a PIL image\n",
    "\t\timg = transforms.functional.to_pil_image(img)\n",
    "\t# Resize the image\n",
    "\timg = img.resize(size)\n",
    "\treturn img\n",
    "\n",
    "def get_feature_vector(img, model, mean_std):\n",
    "\t# Resize the image to the input size of the model\n",
    "\timg = resize(img, (224, 224))\n",
    "\t# Convert the image to a tensor and normalize it using the model's mean and std\n",
    "\timg_tensor = transforms.functional.to_tensor(img)\n",
    "\tif img_tensor.ndim == 4:\n",
    "\t\t# Remove the extra dimensions\n",
    "\t\timg_tensor = img_tensor.squeeze(0)\n",
    "\timg_tensor = img_tensor.view(1, 3, 224, 224)\n",
    "\timg_tensor = transforms.functional.normalize(img_tensor, mean=mean_std[0], std=mean_std[1])\n",
    "\t# Extract the features from the image using the model\n",
    "\tfeatures = model(img_tensor).detach().numpy()\n",
    "\treturn features\n",
    "\n",
    "def calculate_similarity(vector1, vector2):\n",
    "\t# Convert the vectors to PyTorch tensors\n",
    "\tvector1 = torch.tensor(vector1)\n",
    "\tvector2 = torch.tensor(vector2)\n",
    "\t\n",
    "\t# Flatten the tensors to 1D\n",
    "\tvector1 = vector1.view(-1)\n",
    "\tvector2 = vector2.view(-1)\n",
    "\t\n",
    "\t# Calculate the dot product and norms of the two feature vectors\n",
    "\tdot_product = torch.dot(vector1, vector2)\n",
    "\tnorm1 = torch.norm(vector1)\n",
    "\tnorm2 = torch.norm(vector2)\n",
    "\t\n",
    "\t# Calculate the cosine similarity between the two feature vectors\n",
    "\tsimilarity = dot_product / (norm1 * norm2)\n",
    "\treturn similarity.tolist()\n",
    "\n",
    "\n",
    "# Define the mean and standard deviation values for ImageNet\n",
    "# mean = [0.485, 0.456, 0.406]\n",
    "# std = [0.229, 0.224, 0.225]\n",
    "# Define the mean and standard deviation values from my images\n",
    "mean_std = (mean, std)\n",
    "\n",
    "# Create the normalization transform\n",
    "transform = transforms.Compose([\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Load the ResNet50 model from PyTorch's model zoo\n",
    "model = torchvision.models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "# Extract the features from the model's second-to-last fully-connected layer\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Read the anime frame images\n",
    "frame_paths = anime_frames\n",
    "\n",
    "# Extract the features for all anime frames\n",
    "frame_features_list = []\n",
    "for i, frame_path in enumerate(frame_paths):\n",
    "\tframe = cv2.imread(frame_path)\n",
    "\tframe = transform(frame).unsqueeze(0)\n",
    "\tframe_features = get_feature_vector(frame, model, mean_std)\n",
    "\tframe_features_list.append(frame_features)\n",
    "\t# print(f\"{i} frames.\")\n",
    "\n",
    "frame_features_list_squeezed = [np.squeeze(f) for f in frame_features_list]\n",
    "\n",
    "# 11 min\n",
    "# change my normalization technique using this MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar frame for panel mta_data/panels/fma/panel_4.png, mta_data/frames/fma/Pframes_greys/fma_04_i_frame_8888.jpg (similarity: 0.58)\n"
     ]
    }
   ],
   "source": [
    "# Read the manga panel image\n",
    "panel_path = 'mta_data/panels/fma/panel_4.png'\n",
    "panel = cv2.imread(panel_path)\n",
    "panel = transform(panel).unsqueeze(0)\n",
    "panel_features = get_feature_vector(panel, model, mean_std)\n",
    "\n",
    "# Initialize the most similar frame and its similarity score\n",
    "most_similar_frame = None\n",
    "highest_similarity = -1\n",
    "\n",
    "# Compare the manga panel with all anime frames\n",
    "for i, frame_features in enumerate(frame_features_list):\n",
    "\tsimilarity = calculate_similarity(panel_features, frame_features)\n",
    "\tif similarity > highest_similarity:\n",
    "\t\thighest_similarity = similarity\n",
    "\t\tmost_similar_frame = frame_paths[i]\n",
    "\n",
    "# Print the most similar frame and its similarity score\n",
    "print(f\"Most similar frame for panel {panel_path}, {most_similar_frame} (similarity: {highest_similarity:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_path = f\"mta_data/frames/{anime}/Pframes_resnet\"\n",
    "frame_paths = [os.path.join(frame_path, file) for file in os.listdir(frame_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trees 14\n",
      "log num_neighbors 400\n"
     ]
    }
   ],
   "source": [
    "import annoy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def calculate_similarity(vector1, vector2):\n",
    "\t# Convert the vectors to PyTorch tensors\n",
    "\tvector1 = torch.tensor(vector1)\n",
    "\tvector2 = torch.tensor(vector2)\n",
    "\t\n",
    "\t# Flatten the tensors to 1D\n",
    "\tvector1 = vector1.view(-1)\n",
    "\tvector2 = vector2.view(-1)\n",
    "\t\n",
    "\t# Calculate the dot product and norms of the two feature vectors\n",
    "\tdot_product = torch.dot(vector1, vector2)\n",
    "\tnorm1 = torch.norm(vector1)\n",
    "\tnorm2 = torch.norm(vector2)\n",
    "\t\n",
    "\t# Calculate the cosine similarity between the two feature vectors\n",
    "\tsimilarity = dot_product / (norm1 * norm2)\n",
    "\treturn similarity.tolist()\n",
    "\n",
    "def calculate_optimal_trees(n_samples, accuracy):\n",
    "\t# Calculate the optimal number of trees based on the size of the dataset and the desired accuracy\n",
    "\tn_trees = int(np.round(np.log(n_samples) / np.log(2) / accuracy))\n",
    "\treturn n_trees\n",
    "\n",
    "# Build the Annoy index from the frame feature vectors\n",
    "annoy_index = annoy.AnnoyIndex(len(frame_features_list_squeezed[0]), metric='angular')\n",
    "for i, frame_features in enumerate(frame_features_list_squeezed):\n",
    "\tannoy_index.add_item(i, frame_features)\n",
    "\n",
    "# Calculate the sample length of the dataset\n",
    "n_samples = len(frame_features_list_squeezed)\n",
    "\n",
    "# Set the desired accuracy of the search\n",
    "accuracy = 0.9\n",
    "# Set the similarity threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Calculate the optimal number of trees for a dataset with n_samples and an accuracy of accuracy\n",
    "n_trees = calculate_optimal_trees(n_samples, accuracy)\n",
    "print(\"n_trees\", n_trees)\n",
    "annoy_index.build(n_trees)\n",
    "\n",
    "# Set the number of nearest neighbors to search for\n",
    "num_neighbors = int(n_samples * math.log(1 / accuracy) * 0.5)\n",
    "print(\"log num_neighbors\", num_neighbors)\n",
    "\n",
    "# Create a list to store the indexes of the frames to delete\n",
    "frames_to_delete = []\n",
    "\n",
    "# Iterate through the list of frame feature vectors\n",
    "for i, frame_features in enumerate(frame_features_list_squeezed):\n",
    "\t# Check if the current frame has already been added to the list of frames to delete\n",
    "\tif i in frames_to_delete:\n",
    "\t\tcontinue\n",
    "\t# Find the nearest neighbors of the current frame in the Annoy index\n",
    "\tindices = annoy_index.get_nns_by_item(i, num_neighbors)\n",
    "\t# Iterate through the nearest neighbors\n",
    "\tfor j in indices:\n",
    "\t\t# Skip the current frame\n",
    "\t\tif i == j:\n",
    "\t\t\tcontinue\n",
    "\t\t# Calculate the similarity between the current frame and the next nearest neighbor\n",
    "\t\tsimilarity = calculate_similarity(frame_features, frame_features_list_squeezed[j])\n",
    "\t\t# If the similarity is above the threshold, add the nearest neighbor to the list of frames to delete\n",
    "\t\tif similarity > threshold:\n",
    "\t\t\tframes_to_delete.append(j)\n",
    "\n",
    "# Create a list to store the paths of the images to delete\n",
    "image_paths_to_delete = [frame_paths[i] for i in range(len(frame_paths)) if i in frames_to_delete]\n",
    "\n",
    "# Create a ThreadPoolExecutor with a fixed number of threads\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "\t # Submit the delete tasks to the executor\n",
    "\t for image_path in image_paths_to_delete:\n",
    "\t\t executor.submit(os.remove, image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "826286f252ddcff787408661d182f2c63da5c69a1bdcb0cb2e03ac788e5f338e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
